---
# Source: nim-llm/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-svc-account
  labels:
    helm.sh/chart: nim-llm-1.7.0-rc1
    app.kubernetes.io/name: nim-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0.3"
    app.kubernetes.io/managed-by: Helm
---
# Source: nim-llm/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-nim-llm-ssh-pk
type: Opaque
---
# Source: nim-llm/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-nim-llm-scripts-configmap
  labels:
    helm.sh/chart: nim-llm-1.7.0-rc1
    app.kubernetes.io/name: nim-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0.3"
    app.kubernetes.io/managed-by: Helm
data:
  ngc_pull.sh: |-
    #!/bin/bash
    
    set -euo pipefail
    echo "Starting NGC download script. Note: only glibc-based Linux works with NGC CLI -- NOT busybox or alpine"
    
    if [ "$DOWNLOAD_NGC_CLI" = "true" ]; then
      NGC_WD="${DOWNLOAD_NGC_CLI_PATH:-/tmp}"
      if [ ! -x "$(which wget)" ]; then
        echo "To install ngc in the download image, wget is required"
        exit 1
      fi
      wget "https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/${NGC_CLI_VERSION}/files/ngccli_linux.zip" -O "$NGC_WD/ngccli_linux.zip"
      cd "$NGC_WD" && unzip ngccli_linux.zip
      chmod u+x ngc-cli/ngc
      NGC_EXE=$NGC_WD/ngc-cli/ngc
      export PATH=$PATH:$NGC_WD/ngc-cli
    fi
    
    # To ensure we actually have an NGC binary, switch to full path if default is used
    if [ "$NGC_EXE" = "ngc" ]; then
      NGC_EXE=$(which ngc)
    fi
    
    # check if ngc cli is truly available at this point
    if [ ! -x "$NGC_EXE" ]; then
      echo "ngc cli is not installed or available!"
      exit 1
    fi
    
    # download the model
    directory="${STORE_MOUNT_PATH}/${NGC_MODEL_NAME}_v${NGC_MODEL_VERSION}"
    echo "Directory is $directory"
    ready_file="$directory/.ready"
    lockdir="$directory/.lock"
    mkdir -p "$directory"
    set -o noclobber &&
    if { mkdir "$lockdir"; }; then
      trap 'rm -f $lockdir' EXIT
      if [ ! -e "$ready_file" ]; then
        $NGC_EXE registry model download-version --dest "$STORE_MOUNT_PATH" "${NGC_CLI_ORG}/${NGC_CLI_TEAM}/${NGC_MODEL_NAME}:${NGC_MODEL_VERSION}"
        # decrypt the model - if needed (conditions met)
        if [ -n "${NGC_DECRYPT_KEY:+''}" ] && [ -f "$directory/${MODEL_NAME}.enc" ]; then
          echo "Decrypting $directory/${MODEL_NAME}.enc"
          # untar if necessary
          if [ -n "${TARFILE:+''}" ]; then
            echo "TARFILE enabled, unarchiving..."
            openssl enc -aes-256-cbc -d -pbkdf2 -in "$directory/${MODEL_NAME}.enc" -out "$directory/${MODEL_NAME}.tar" -k "${NGC_DECRYPT_KEY}"
            tar -xvf "$directory/${MODEL_NAME}.tar" -C "$STORE_MOUNT_PATH"
            rm "$directory/${MODEL_NAME}.tar"
          else
            openssl enc -aes-256-cbc -d -pbkdf2 -in "$directory/${MODEL_NAME}.enc" -out "$directory/${MODEL_NAME}" -k "${NGC_DECRYPT_KEY}"
          fi
          rm "$directory/${MODEL_NAME}.enc"
        else
          echo "No decryption key provided, or encrypted file found. Skipping decryption.";
          if [ -n "${TARFILE:+''}" ]; then
            echo "TARFILE enabled, unarchiving..."
            tar -xvf "$directory/${NGC_MODEL_VERSION}.tar.gz" -C "$STORE_MOUNT_PATH"
            rm "$directory/${NGC_MODEL_VERSION}.tar.gz"
          fi
        fi
        touch "$ready_file"
        echo "Done dowloading"
        rmdir "$lockdir"
      else
        echo "Download was already complete"
      fi;
    else
      while [ ! -e "$ready_file" ]
      do
        echo "Did not get the download lock. Waiting for the pod holding the lock to download the files."
        sleep 1
      done;
      echo "Done waiting"
    fi
    set +o noclobber;
    ls -la "$directory"
---
# Source: nim-llm/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-nim-llm-lws-mpi-config
  labels:
    helm.sh/chart: nim-llm-1.7.0-rc1
    app.kubernetes.io/name: nim-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0.3"
    app.kubernetes.io/managed-by: Helm
data:
  hostfile-0: |
    localhost slots=1
---
# Source: nim-llm/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-nim-llm-mpi-start-script
  labels:
    helm.sh/chart: nim-llm-1.7.0-rc1
    app.kubernetes.io/name: nim-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0.3"
    app.kubernetes.io/managed-by: Helm
data:
  start-mpi-cluster.sh: |-
    #!/bin/bash

    NIM_JSONL_LOGGING="${NIM_JSONL_LOGGING:-0}"
    if [ "${NIM_JSONL_LOGGING}" -eq "1" ] && /opt/nim/llm/.venv/bin/python3 -c "import nim_llm_sdk.logging.pack_all_logs_into_json" 2> /dev/null; then
      /opt/nim/llm/.venv/bin/python3 -m nim_llm_sdk.entrypoints.openai.api_server |& /opt/nim/llm/.venv/bin/python3 -m nim_llm_sdk.logging.pack_all_logs_into_json
    else
      /opt/nim/llm/.venv/bin/python3 -m nim_llm_sdk.entrypoints.openai.api_server
    fi
---
# Source: nim-llm/templates/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: release-name-nim-llm
  labels:
    helm.sh/chart: nim-llm-1.7.0-rc1
    app.kubernetes.io/name: nim-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0.3"
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - "ReadWriteMany"
  resources:
    requests:
      storage: "30Gi"
  storageClassName: "local-nfs"
---
# Source: nim-llm/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: foobar
  labels:
    helm.sh/chart: nim-llm-1.7.0-rc1
    app.kubernetes.io/name: nim-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0.3"
    app.kubernetes.io/managed-by: Helm
  annotations:
    blue.com/example: "no"
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: http-openai
      name: http-openai
  selector:
    app.kubernetes.io/name: nim-llm
    app.kubernetes.io/instance: release-name
    nim-llm-role: "leader"
---
# Source: nim-llm/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: release-name-nim-llm
  labels:
    helm.sh/chart: nim-llm-1.7.0-rc1
    app.kubernetes.io/name: nim-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0.3"
    app.kubernetes.io/managed-by: Helm
spec:
  ingressClassName: ingress-awesome
  rules:
    - host: "chart-example.local"
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: "release-name-nim-llm-openai"
                port:
                  number: 8000
---
# Source: nim-llm/templates/multi_node_deployment.yaml
apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: lws-nim-llm
  labels:
    helm.sh/chart: nim-llm-1.7.0-rc1
    app.kubernetes.io/name: nim-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0.3"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 1
    restartPolicy: RecreateGroupOnPodRestart
    leaderTemplate:
      metadata:
        labels:
          helm.sh/chart: nim-llm-1.7.0-rc1
          app.kubernetes.io/name: nim-llm
          app.kubernetes.io/instance: release-name
          app.kubernetes.io/version: "1.0.3"
          app.kubernetes.io/managed-by: Helm
          nim-llm-role: "leader"
        annotations:
          {}
      spec:
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsUser: 1000
        containers:
          - name: nim-leader
            image: "nvcr.io/nim/meta/llama3-8b-instruct:latest"
            env:
              - name: NIM_LEADER_ROLE
                value: "true"
              - name: NIM_MPI_ALLOW_RUN_AS_ROOT
                value: "0"
              - name: OMPI_MCA_orte_keep_fqdn_hostnames
                value: "true"
              - name: OMPI_MCA_plm_rsh_args
                value: "-o ConnectionAttempts=20"
              - name: NIM_CACHE_PATH
                value: "/model-store"
              - name: NGC_API_KEY
                valueFrom:
                  secretKeyRef:
                    name: ngc-api
                    key: NGC_API_KEY
              - name: NIM_SERVER_PORT
                value: "8000"
              - name: NIM_JSONL_LOGGING
                value: "1"
              - name: NIM_LOG_LEVEL
                value: "INFO"
              - name: NIM_NUM_COMPUTE_NODES
                value: "1"
              - name: GPUS_PER_NODE
                value: "1"
              - name: CLUSTER_START_TIMEOUT
                value: "300"
              - name: CLUSTER_SIZE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['leaderworkerset.sigs.k8s.io/size']
              - name: GROUP_INDEX
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.labels['leaderworkerset.sigs.k8s.io/group-index']
            ports:
              
              - containerPort: 8000
                name: http-openai
            resources:
              limits:
                nvidia.com/gpu: 1
              requests:
                nvidia.com/gpu: 1
            volumeMounts:
              - name: model-store
                mountPath: /model-store
              - mountPath: /dev/shm
                name: dshm
              - name: scripts-volume
                mountPath: /scripts
              - name: ssh-pk
                mountPath: /opt/nim/llm/pk
              - name: ssh-dotfiles
                mountPath: /opt/nim/llm/.ssh
              - name: mpi-config
                mountPath: /etc/mpi
              - name: start-mpi-script
                mountPath: /opt/nim/start-mpi-cluster.sh
                subPath: start-mpi-cluster.sh            
            livenessProbe:
              httpGet:
                path: /v1/health/live
                port: http-openai
              initialDelaySeconds: 15
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /v1/health/ready
                port: http-openai
              initialDelaySeconds: 15
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 3
            startupProbe:
              httpGet:
                path: /v1/health/ready
                port: http-openai
              initialDelaySeconds: 40
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 180
        tolerations:
          - effect: NoSchedule
            key: nvidia.com/gpu
            operator: Exists
        imagePullSecrets:
          - name: gitlab-imagepull
        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
          - name: scripts-volume
            configMap:
              name: release-name-nim-llm-scripts-configmap
              defaultMode: 0555
          - name: model-store
            persistentVolumeClaim:
              claimName:  release-name-nim-llm
          - name: mpi-config
            configMap:
              name: release-name-nim-llm-lws-mpi-config
              defaultMode: 0444
          - name: start-mpi-script
            configMap:
              name: release-name-nim-llm-mpi-start-script
              defaultMode: 0555
          - name: ssh-dotfiles
            emptyDir: {}
          - name: ssh-pk
            secret:
              defaultMode: 256
              secretName: release-name-nim-llm-ssh-pk
    workerTemplate:
      metadata:
        annotations:
          {}
      spec:
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsUser: 1000
        containers:
          - name: nim-worker
            image: "nvcr.io/nim/meta/llama3-8b-instruct:latest"
            ports:
              
              - containerPort: 8000
                name: http-openai
            resources:
              limits:
                nvidia.com/gpu: 1
              requests:
                nvidia.com/gpu: 1
            volumeMounts:
              - name: model-store
                mountPath: /model-store
              - mountPath: /dev/shm
                name: dshm
              - name: scripts-volume
                mountPath: /scripts
              - name: ssh-confs
                mountPath: /ssh-confs
              - name: ssh-dotfiles
                mountPath: /opt/nim/llm/.ssh
              - name: ssh-pk
                mountPath: /opt/nim/llm/pk
              - name: start-mpi-script
                mountPath: /opt/nim/start-mpi-cluster.sh
                subPath: start-mpi-cluster.sh
            env:
              - name: NIM_LEADER_ROLE
                value: "false"
              - name: NIM_MPI_ALLOW_RUN_AS_ROOT
                value: "0"
              - name: NIM_CACHE_PATH
                value: "/model-store"
              - name: NGC_API_KEY
                valueFrom:
                  secretKeyRef:
                    name: ngc-api
                    key: NGC_API_KEY
              - name: NIM_SERVER_PORT
                value: "8000"
              - name: NIM_JSONL_LOGGING
                value: "1"
              - name: NIM_LOG_LEVEL
                value: "INFO"
              - name: NIM_NUM_COMPUTE_NODES
                value: "1"
              
              - name: LEADER_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['leaderworkerset.sigs.k8s.io/leader-name']
              - name: NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              - name: LWS_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.labels['leaderworkerset.sigs.k8s.io/name']
        tolerations:
          - effect: NoSchedule
            key: nvidia.com/gpu
            operator: Exists
        imagePullSecrets:
          - name: gitlab-imagepull
        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
          - name: scripts-volume
            configMap:
              name: release-name-nim-llm-scripts-configmap
              defaultMode: 0555
          - name: model-store
            persistentVolumeClaim:
              claimName:  release-name-nim-llm
          - name: ssh-confs
            emptyDir: {}
          - name: ssh-dotfiles
            emptyDir: {}
          - name: ssh-pk
            secret:
              defaultMode: 256
              secretName: release-name-nim-llm-ssh-pk
          - name: start-mpi-script
            configMap:
              name: release-name-nim-llm-mpi-start-script
              defaultMode: 0555
---
# Source: nim-llm/templates/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-nim-llm
  labels:
    helm.sh/chart: nim-llm-1.7.0-rc1
    app.kubernetes.io/name: nim-llm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0.3"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: nim-llm
      app.kubernetes.io/instance: release-name
  endpoints:
  - port: http-openai
